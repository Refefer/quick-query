#!/usr/bin/env python3
from typing import Optional, Dict, List, Any, Generator
from pathlib import Path
import readline
import argparse
import os
import requests
import select
import sys
import tomllib
import json

def load_toml_file(
    file_path: str
) -> Dict[str, Any]:
    """
    Load and parse a TOML file into a dictionary.

    Args:
        file_path: Path to the TOML file

    Returns:
        Dictionary of parsed TOML data or empty dict if error
    """
    path = os.path.expanduser(file_path)
    if not os.path.exists(path):
        print(f"Error: File not found: {path}")
        return {}

    try:
        with open(path, "rb") as f:
            return tomllib.load(f)

    except tomllib.TOMLDecodeError:
        print(f"Error: Invalid TOML format in {path}")
        return {}


def read_api_conf(
    config_path: str,
    namespace: Optional[str]
) -> Dict[str, Any]:
    """
    Read API configuration from a TOML file for a given namespace.

    Args:
        config_path: Path to the configuration file
        namespace: Section name in the TOML file

    Returns:
        Dict of configuration values
    """
    conf = load_toml_file(config_path)
    ns = namespace or "default"
    return conf.get(ns, {})


def load_toml_prompt(
    file_path: str,
    section_name: Optional[str]
) -> Optional[str]:
    """
    Load a prompt string from a TOML file section.

    Args:
        file_path: Path to the TOML file
        section_name: Name of the section to retrieve

    Returns:
        Prompt string if found, else None
    """
    if not section_name:
        return None

    data = load_toml_file(file_path)
    section = data.get(section_name, {})
    return section.get("prompt")

def build_headers(
    api_key: str
) -> Dict[str, str]:
    """
    Build standard HTTP headers for API requests.

    Args:
        api_key: Authentication key

    Returns:
        Dictionary of HTTP headers
    """
    return {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

def api_request(
    method: str,
    url: str,
    api_key: str,
    json_payload: Optional[Dict[str, Any]] = None
) -> Optional[Dict[str, Any]]:
    """
    Perform a generic API request.

    Args:
        method: HTTP method, "GET" or "POST"
        url: Full request URL
        api_key: Authentication token
        json_payload: JSON data for POST requests

    Returns:
        Parsed JSON response or None on failure
    """
    headers = build_headers(api_key)
    try:
        if method.upper() == "GET":
            response = requests.get(url, headers=headers)
        else:
            response = requests.post(url, headers=headers, json=json_payload)

        response.raise_for_status()
        return response.json()

    except requests.RequestException as e:
        print(f"API Error: {e}")
        return None


def get_model_id(
    api_url: str,
    api_key: str
) -> Optional[str]:
    """
    Retrieve the first available model ID from the API.

    Args:
        api_url: Base API URL
        api_key: Authentication key

    Returns:
        Model ID string or None if not found
    """
    result = api_request("GET", f"{api_url}/models", api_key)
    if not result:
        return None

    data = result.get("data", [])
    if data and isinstance(data, list):
        return data[0].get("id")

    return None

def send_chat_completion(
    host: str,
    api_key: str,
    model: str,
    messages: List[Dict[str, str]]
) -> Generator[str, None, None]:
    """
    Streams responses from an OpenAI-compatible chat/completions endpoint in real-time.
    Streams part of the data to stderr until a substring is found, then switches to stdout.

    Parameters:
    url (str): The API endpoint URL of the server.
    model (str): The model name to use for the request.
    prompt (str): The input prompt for the model.
    api_key (str): The API key for authentication.
    switch_substring (str): The substring to trigger switching to stdout. Default is '</cot>'.

    Yields:
    Response object
    """
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "messages": messages,
        "stream": True
    }
    url = f"{host}/chat/completions"
    return requests.post(url, headers=headers, json=data, stream=True)

def stream_chunks(response, min_chunk_size=None):
    buffer = ''
    for line in response.iter_lines():
        if not line:
            continue

        line_data = line.decode('utf-8')
        if line_data.startswith('data: '):
            content = line_data[6:]
            if content == '[DONE]':
                if buffer:
                    yield buffer
                break

            try:
                json_data = json.loads(content)
                chunk = json_data['choices'][0]['delta'].get('content', '')
            except Exception as e:
                print(f"Error parsing line: {e}")
                sys.exit(1)

            if min_chunk_size is None:
                yield chunk
            else:
                buffer = chunk if len(buffer) == 0 else buffer + chunk
                if len(buffer) >= min_chunk_size:
                    yield buffer
                    buffer = ''

def create_prefixes(think_tag):
    start_tag = f'<{think_tag}>'
    end_tag = f'</{think_tag}>'
    prefixes = set()
    for tag in (start_tag, end_tag):
        for i in range(1, len(tag) - 1):
            prefixes.add(tag[:i])

    return tuple(prefixes)

def stream_tokens(
    response, 
    think_tag=None,
    min_chunk_size=None
):
    stream = stream_chunks(response, min_chunk_size)
    if think_tag is None:
        yield from stream
    else:
        prefixes = create_prefixes(think_tag)
        buffer = ''
        for chunk in stream:
            buffer = chunk if len(buffer) == 0 else buffer + chunk
            if not buffer.endswith(prefixes):
                yield buffer
                buffer = ''

        if len(buffer) > 0:
            yield buffer

def stream_cot_blocks(
    response,
    think_tag=None,
    min_chunk_size=None,
    cr_after_end_block=True
):
    stream = stream_tokens(response, think_tag, min_chunk_size)
    if think_tag is None:
        for c in stream:
            yield False, c
    else:
        start_block, end_block = f'<{think_tag}>', f'</{think_tag}>'
        in_block = False
        for chunk in stream:
            buffer = chunk
            while buffer:
                if start_block in buffer:
                    before, after = chunk.split(start_block, 1)
                    yield in_block, before
                    in_block = True
                    yield in_block, start_block
                    buffer = after
                    continue

                if end_block in buffer:
                    before, after = chunk.split(end_block, 1)
                    yield in_block, before
                    yield in_block, end_block
                    if cr_after_end_block:
                        yield in_block, '\n'

                    in_block = False
                    buffer = after
                    continue

                yield in_block, buffer
                buffer = ''

def try_read_stdin() -> str | None:
    """Try to read from standard input without blocking.
    
    Uses select to check if input is available. Returns the input if available,
    otherwise returns None.
    """
    if select.select([sys.stdin], [], [], 0)[0]:
        return sys.stdin.read()
    else:
        return None

def process_stream(
    response, 
    cot_token, 
    min_chunk_size, 
    needs_buffering, 
    formatter
):
    token_stream = stream_cot_blocks(response, cot_token, min_chunk_size)
    response = []
    if needs_buffering:
        out_message = []
        for in_block, chunk in token_stream:
            response.append(chunk)
            if in_block:
                formatter.print_in_block(chunk)
            else:
                out_message.append(chunk)

        formatter.print_out_block(''.join(out_message))
    else:
        for in_block, chunk in token_stream:
            response.append(chunk)
            if in_block:
                formatter.print_in_block(chunk)
            else:
                formatter.print_out_block(chunk)

    return ''.join(response)

def get_formatter(args):
    in_block_path = args.cot_block_fd
    if args.format_markdown:
        return MarkdownFormatter(in_block_path, sys.stdout)
    else:
        return RawTextFormatter(in_block_path, sys.stdout)

def chat(
    args, 
    model_id, 
    messages, 
    cot_token, 
    min_chunk_size = 20
):
    formatter = get_formatter(args)
    orig_message_len = len(messages)
    try:
        while True:
            user_input = input("> ").strip()
            print('=' * 10)
            
            if len(user_input) == 0:
                continue

            maybe_command = user_input.split(None, 1)[0].lower()
            match maybe_command:
                case "/reset":
                    messages = messages[:orig_message_len]
                    continue

                case "/save":
                    path = user_input[6:]
                    with open(path, 'w') as out:
                        for message in messages:
                            print(f"Role: {message['role']}\n{message['content']}\n", file=out)

                    print(f"Saved to {path}")
                    continue

                case "/undo":
                    if len(message) > 2:
                        messages = message[:-2]

                    continue

                case "/redo":
                    if len(messages) > 2:
                        user_input = messages[-2]['content']
                        messages = messages[:-2]

                    continue


            messages.append({"role": "user", "content": user_input})
            response = send_chat_completion(
                args.host,
                args.api_key,
                model_id,
                messages
            )

            response = process_stream(response, cot_token, min_chunk_size, args.format_markdown, formatter)
            messages.append({"role": "assistant", "content": response})
            if not response.endswith('\n'):
                print()

            print('=' * 10)

    except (KeyboardInterrupt, EOFError):
        print("\nExiting chat.")

def run_prompt(
    args, 
    user_prompt, 
    model_id, 
    messages, 
    cot_token,
    min_chunk_size = 20
):
    prompt = args.prompt
    stdin_prompt = try_read_stdin()
    if stdin_prompt is not None:
        prompt = f"{prompt}\n\n{stdin_prompt}".strip()

    if user_prompt:
        prompt = f"{user_prompt}\n{prompt}"

    messages.append({"role": "user", "content": prompt})

    response = send_chat_completion(
        args.host,
        args.api_key,
        model_id,
        messages
    )

    formatter = get_formatter(args)
    process_stream(response, cot_token, min_chunk_size, args.format_markdown, formatter)

class MessageFormatter:
    def __init__(self, in_block, out_block):
        if isinstance(in_block, str):
            in_block = open(in_block, 'w')

        self.in_block = in_block
        if isinstance(in_block, str):
            out_block = open(out_block, 'w')

        self.out_block = out_block

    def print_in_block(self, message: str):
        """Takes a message, formats it for terminal output, and prints it."""
        raise NotImplementedError()

    def print_out_block(self, message: str):
        """Takes a message, formats it for terminal output, and prints it."""
        raise NotImplementedError()


class RawTextFormatter(MessageFormatter):
    """Formats messages as raw text."""
    def print_in_block(self, message: str):
        """Prints the message as raw text."""
        self.in_block.write(message)
        self.in_block.flush()

    def print_out_block(self, message: str):
        self.out_block.write(message)
        self.out_block.flush()

class MarkdownFormatter(MessageFormatter):
    """Formats messages as markdown using the rich library."""

    def __init__(self, in_block, out_block):
        """Formats and prints the message as markdown."""
        super().__init__(in_block, out_block)
        from rich.console import Console
        self.out_block_console = Console(file=self.out_block)

    def print_in_block(self, message: str):
        """Formats and prints the message as markdown."""
        self.in_block.write(message)
        self.in_block.flush()

    def print_out_block(self, message: str):
        from rich.markdown import Markdown
        markdown = Markdown(message)
        self.out_block_console.print(markdown)

def list_prompts(args):
    print("Available system prompts:")
    system_data = load_toml_file(args.system_prompt_file)
    for key in system_data:
        print(f"  {key}")

    print("\nAvailable user prompts:")
    user_data = load_toml_file(args.user_prompt_file)
    for key in user_data:
        print(f"  {key}")

def main(args) -> None:
    """
    Main execution flow for the script.
    """

    if args.host is None:
        conf = read_api_conf(
            args.conf_file,
            args.server
        )
        args.host = conf.get("host")
        args.api_key = conf.get("api_key")
        args.model = args.model or conf.get("model")

    system_prompt = None
    if args.system_prompt_file and args.system_prompt_name:
        system_prompt = load_toml_prompt(
            args.system_prompt_file,
            args.system_prompt_name
        )
        if not system_prompt:
            return

    user_prompt = None
    if args.user_prompt_file and args.user_prompt_name:
        user_prompt = load_toml_prompt(
            args.user_prompt_file,
            args.user_prompt_name
        )
        if not user_prompt:
            return

    if args.model is None:
        model_id = get_model_id(
            args.host,
            args.api_key
        )
        if not model_id:
            return
    else:
        model_id = args.model

    messages: List[Dict[str, str]] = []
    if system_prompt:
        messages.append(
            {"role": "system", "content": system_prompt}
        )

    cot_token = 'think'
    if args.chat:
        chat(args, model_id, messages, cot_token)
    else:
        run_prompt(args, user_prompt, model_id, messages, cot_token)
        
CONFIG_DIR: Path = Path(os.environ.get("XDG_CONFIG_HOME", Path.home() / ".config")) / "quick-query"
SYSTEM_PROMPT_FILE: Path = CONFIG_DIR / "prompts.toml"
USER_PROMPT_FILE: Path = CONFIG_DIR / "user_prompts.toml"
CONF_FILE: Path = CONFIG_DIR / "conf.toml"

def parse_arguments() -> argparse.Namespace:
    """
    Parse command-line arguments.

    Returns:
        argparse.Namespace: Parsed arguments.
    """
    parser: argparse.ArgumentParser = argparse.ArgumentParser(
        description="Query an OpenAI-compatible endpoint"
    )
    parser.add_argument(
        "-p",
        "--prompt",
        required=not "-c" in sys.argv and not '--list-prompts' in sys.argv,
        help="The user's prompt"
    )
    parser.add_argument(
        "--system-prompt-file",
        default=str(SYSTEM_PROMPT_FILE),
        help="Path to TOML file containing system prompts"
    )
    parser.add_argument(
        "-sp",
        "--system-prompt-name",
        default=None,
        help="Name of the system prompt section in the TOML file"
    )
    parser.add_argument(
        "--user-prompt-file",
        default=str(USER_PROMPT_FILE),
        help="Path to TOML file containing user prompts"
    )
    parser.add_argument(
        "--user-prompt-name",
        default=None,
        help="Name of the user prompt section in the TOML file"
    )
    parser.add_argument(
        "--conf-file",
        default=str(CONF_FILE),
        help="Path to TOML file containing configuration"
    )
    parser.add_argument(
        "-s",
        dest="server",
        default="default",
        help="Name of the server to connect to in conf.toml"
    )
    parser.add_argument(
        "--host",
        default=None,
        help="API endpoint base URL"
    )
    parser.add_argument(
        "--api-key",
        default=None,
        help="API key for authentication"
    )
    parser.add_argument(
        "--model",
        default=None,
        help="Model identifier"
    )
    parser.add_argument(
        "-c",
        "--chat",
        action="store_true",
        help="Enter interactive chat mode"
    )
    parser.add_argument(
        "-m",
        "--format-markdown",
        action="store_true",
        help="If enabled, formats output for the terminal in markdown.  If the library isn't installed, prints as text"
    )
    parser.add_argument(
        "--cot-block-fd",
        default='/dev/tty',
        help="Where to emit cot blocks.  Default is /dev/tty."
    )
    parser.add_argument(
        "--list-prompts",
        action="store_true",
        help="List all available system and user prompts and exit"
    )

    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    if args.list_prompts:
        list_prompts(args)
        sys.exit(0)

    main(args)
